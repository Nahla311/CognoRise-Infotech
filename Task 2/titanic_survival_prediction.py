# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nXMp87IlfU2ejoDvF5YdYxu36ENmt4im
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

data = pd.read_csv('/content/tested.csv')

data.head()

sns.set_style('whitegrid')

# Data Visualization
plt.figure(figsize=(10,6))
img1 = sns.countplot(x='Survived', hue="Sex", data = data)
for p in img1.patches:
    img1.annotate(p.get_height(),(p.get_x()+0.15, p.get_height()+1))

plt.figure(figsize=(10,6))
img2 = sns.countplot(x='Survived', hue="Pclass", data = data)
for p in img2.patches:
    img2.annotate(p.get_height(),(p.get_x()+0.15, p.get_height()+1))

sns.boxplot(x='Sex', y='Age', data = data, hue='Pclass')
plt.legend(loc='upper left',bbox_to_anchor=(1,1))

# Missing data
sns.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='plasma')

#Data Cleaning
#Age

def name_prefix(str):
    str_prefix = re.findall(r'(M(?:is)?s\.|(?:Mrs\.)|(?:Mr\.)|(?:Master\.)|(?:Rev\.)|(?:Dr\.))',str)
    if (str_prefix != []):
        return str_prefix[0]
    else:
        return 'None'

data['Name_Prefix'] = data['Name'].apply(name_prefix)

plt.figure(figsize = (10,6))
sns.boxplot(x='Name_Prefix', y='Age', data = data, hue='Pclass')
plt.legend(loc='upper left',bbox_to_anchor=(1,1))

def cal_age(val):
    Age = val[0]
    Pclass = val[1]
    Name_Prefix = val[2]

    if pd.isnull(Age):
        if Pclass == 1:
            if Name_Prefix == 'Mr.':
                return 40
            elif Name_Prefix == 'Mrs.':
                return 41
            elif Name_Prefix == 'Miss.':
                return 30
            elif Name_Prefix == 'Master.':
                return 4
            elif Name_Prefix == 'Dr.':
                return 46
            elif Name_Prefix == 'Ms.':
                return 28
            else:
                return 46

        elif Pclass == 2:
            if Name_Prefix == 'Mr.':
                return 30
            elif Name_Prefix == 'Mrs.':
                return 32
            elif Name_Prefix == 'Miss.':
                return 24
            elif Name_Prefix == 'Master.':
                return 2
            elif Name_Prefix == 'Dr.':
                return 38
            elif Name_Prefix == 'Ms.':
                return 28
            else:
                return 46
        else:
            if Name_Prefix == 'Mr.':
                return 25
            elif Name_Prefix == 'Mrs.':
                return 13
            elif Name_Prefix == 'Miss.':
                return 18
            elif Name_Prefix == 'Master.':
                return 4
            elif Name_Prefix == 'Dr.':
                return 42
            elif Name_Prefix == 'Ms.':
                return 28
            else:
                return 46

    else:
        return Age

data['Age'] = data[['Age','Pclass','Name_Prefix']].apply(cal_age,axis=1)

# Cabin Column
data.drop(columns = 'Cabin',inplace= True)

# Drop rows that has null values
data.dropna(axis=0, inplace=True)

# Converting Categorical Feature into dummy variables
sex = pd.get_dummies(data['Sex'], drop_first=True)
embark = pd.get_dummies(data['Embarked'], drop_first=True)
name_prefix = pd.get_dummies(data['Name_Prefix'], drop_first=True)

data.drop(['Sex','Embarked','Name','Ticket','Name_Prefix'],axis=1,inplace=True)

titanic_train = pd.concat([data,sex,embark,name_prefix], axis=1)

titanic_train.info()

"""# Train Test Split"""

X = titanic_train.drop('Survived', axis=1)

y = titanic_train['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

# Machine Learning algorithms

# Logistic Regression

logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)

predict = logmodel.predict(X_test)
print(classification_report(y_test, predict))

# Random Forest


rfc = RandomForestClassifier(n_estimators=400)
rfc.fit(X_train,y_train)

pred = rfc.predict(X_test)
print(classification_report(y_test,pred))